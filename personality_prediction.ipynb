{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personality Prediction Project: Understanding People with Machine Learning\n",
    "\n",
    "## 1. Introduction\n",
    "Welcome to this project! We're going to explore how machine learning can help us understand different personality types. We'll use three simple but powerful methods: Decision Trees, Random Forests, and AdaBoost. Don't worry if these sound complicated; we'll explain everything clearly. Our main goal is to build models that can predict a person's personality type based on some characteristics, and then see which method works best.\n",
    "\n",
    "## 2. Our Data: Understanding Personality\n",
    "We're working with a dataset called `Data.csv`. This file contains information about many different people. Each person is described by various traits, like how much social energy they have, if they prefer alone time, how talkative they are, and many more. The most important piece of information for us is their `personality_type`, which is what we want to predict.\n",
    "\n",
    "### What Our Data Looks Like\n",
    "Our `Data.csv` file has columns representing different traits. For example:\n",
    "*   `social_energy`: How much energy a person gets from social interactions.\n",
    "*   `alone_time_preference`: How much a person enjoys being alone.\n",
    "*   `talkativeness`: How much a person talks.\n",
    "*   ...and many more traits.\n",
    "\n",
    "The `personality_type` column tells us if a person is an 'Extrovert', 'Introvert', or 'Ambivert'. This is what our models will try to guess.\n",
    "\n",
    "### Preparing Our Data\n",
    "Before we can teach our computers to predict personality types, we need to prepare the data. This involves a few steps:\n",
    "*   **Separating Traits and Personality Type**: We'll separate all the trait columns (our 'features') from the `personality_type` column (our 'target').\n",
    "*   **Splitting the Data**: We'll divide our data into three parts: \n",
    "    *   **Training Data**: This is what our models will learn from.\n",
    "    *   **Validation Data**: We'll use this to fine-tune our models and pick the best one.\n",
    "    *   **Test Data**: This is like a final exam for our best model. It's data the model has never seen before, so it gives us a true idea of how well it works in the real world.\n",
    "    We'll split the data so that about 60% is for training, 30% for validation, and 10% for testing. This helps make sure our models are fair and accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier Implementation From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeNode:\n",
    "    \"\"\"Node class for decision tree - represents either a decision point or leaf\"\"\"\n",
    "    def __init__(self):\n",
    "        self.feature_idx = None     # Feature index for split\n",
    "        self.threshold = None       # Split threshold\n",
    "        self.left = None            # Left child\n",
    "        self.right = None           # Right child\n",
    "        self.value = None           # Predicted class (leaf nodes)\n",
    "        self.is_leaf = False        # Leaf indicator\n",
    "\n",
    "class DecisionTreeClassifier:\n",
    "    \"\"\"Decision Tree Classifier using Gini impurity for splits\"\"\"\n",
    "\n",
    "    def __init__(self, max_depth=None, min_samples_leaf=1, random_state=42, max_features=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.random_state = random_state\n",
    "        self.max_features = max_features\n",
    "        self.root = None\n",
    "        self.classes_ = None\n",
    "        self.n_features_ = None\n",
    "\n",
    "        if random_state is not None:\n",
    "            np.random.seed(random_state)\n",
    "\n",
    "    def _gini_impurity(self, y):\n",
    "        \"\"\"Calculate Gini impurity: 1 - Σ(p_i)²\"\"\"\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "        proportions = np.bincount(y) / len(y)\n",
    "        return 1 - np.sum(proportions ** 2)\n",
    "\n",
    "    def _information_gain(self, y, left_y, right_y):\n",
    "        \"\"\"Calculate information gain from split\"\"\"\n",
    "        n = len(y)\n",
    "        n_left, n_right = len(left_y), len(right_y)\n",
    "\n",
    "        if n_left == 0 or n_right == 0:\n",
    "            return 0\n",
    "\n",
    "        parent_gini = self._gini_impurity(y)\n",
    "        weighted_child_gini = (n_left / n) * self._gini_impurity(left_y) + \\\n",
    "                             (n_right / n) * self._gini_impurity(right_y)\n",
    "\n",
    "        return parent_gini - weighted_child_gini\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        \"\"\"Find best feature and threshold for splitting\"\"\"\n",
    "        best_gain = 0\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        # Feature selection for Random Forest\n",
    "        if self.max_features is not None and self.max_features < n_features:\n",
    "            feature_indices = np.random.choice(n_features, self.max_features, replace=False)\n",
    "        else:\n",
    "            feature_indices = range(n_features)\n",
    "\n",
    "        for feature_idx in feature_indices:\n",
    "            feature_values = X[:, feature_idx]\n",
    "            thresholds = np.unique(feature_values)\n",
    "\n",
    "            for threshold in thresholds:\n",
    "                left_mask = feature_values <= threshold\n",
    "                right_mask = ~left_mask\n",
    "\n",
    "                if np.sum(left_mask) < self.min_samples_leaf or \\\n",
    "                   np.sum(right_mask) < self.min_samples_leaf:\n",
    "                    continue\n",
    "\n",
    "                left_y = y[left_mask]\n",
    "                right_y = y[right_mask]\n",
    "\n",
    "                gain = self._information_gain(y, left_y, right_y)\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature_idx\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        return best_feature, best_threshold, best_gain\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        \"\"\"Recursively build decision tree\"\"\"\n",
    "        node = DecisionTreeNode()\n",
    "\n",
    "        # Stopping criteria\n",
    "        if (self.max_depth is not None and depth >= self.max_depth) or \\\n",
    "           len(np.unique(y)) == 1 or \\\n",
    "           len(y) < 2 * self.min_samples_leaf:\n",
    "\n",
    "            node.is_leaf = True\n",
    "            node.value = Counter(y).most_common(1)[0][0]\n",
    "            return node\n",
    "\n",
    "        best_feature, best_threshold, best_gain = self._best_split(X, y)\n",
    "\n",
    "        if best_feature is None or best_gain == 0:\n",
    "            node.is_leaf = True\n",
    "            node.value = Counter(y).most_common(1)[0][0]\n",
    "            return node\n",
    "\n",
    "        left_mask = X[:, best_feature] <= best_threshold\n",
    "        right_mask = ~left_mask\n",
    "\n",
    "        node.feature_idx = best_feature\n",
    "        node.threshold = best_threshold\n",
    "\n",
    "        node.left = self._build_tree(X[left_mask], y[left_mask], depth + 1)\n",
    "        node.right = self._build_tree(X[right_mask], y[right_mask], depth + 1)\n",
    "\n",
    "        return node\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train the decision tree\"\"\"\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        if isinstance(y, pd.Series):\n",
    "            y = y.values\n",
    "\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.n_features_ = X.shape[1]\n",
    "\n",
    "        # Create label mapping\n",
    "        self.label_to_int = {label: i for i, label in enumerate(self.classes_)}\n",
    "        self.int_to_label = {i: label for label, i in self.label_to_int.items()}\n",
    "\n",
    "        y_int = np.array([self.label_to_int[label] for label in y])\n",
    "\n",
    "        self.root = self._build_tree(X, y_int)\n",
    "        return self\n",
    "\n",
    "    def _predict_sample(self, x):\n",
    "        \"\"\"Predict single sample\"\"\"\n",
    "        node = self.root\n",
    "\n",
    "        while not node.is_leaf:\n",
    "            if x[node.feature_idx] <= node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "\n",
    "        return self.int_to_label[node.value]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict multiple samples\"\"\"\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "\n",
    "        return np.array([self._predict_sample(x) for x in X])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Implementation From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestClassifier:\n",
    "    \"\"\"Random Forest using bootstrap sampling and feature randomness\"\"\"\n",
    "\n",
    "    def __init__(self, n_estimators=100, max_depth=None, min_samples_leaf=1,\n",
    "                 max_features='sqrt', random_state=42):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features\n",
    "        self.random_state = random_state\n",
    "        self.trees = []\n",
    "        self.classes_ = None\n",
    "        self.n_features_ = None\n",
    "\n",
    "        if random_state is not None:\n",
    "            np.random.seed(random_state)\n",
    "\n",
    "    def _bootstrap_sample(self, X, y):\n",
    "        \"\"\"Create bootstrap sample\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        bootstrap_indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "        return X[bootstrap_indices], y[bootstrap_indices]\n",
    "\n",
    "    def _get_max_features(self, n_features):\n",
    "        \"\"\"Calculate number of features per split\"\"\"\n",
    "        if self.max_features == 'sqrt':\n",
    "            return int(np.sqrt(n_features))\n",
    "        elif self.max_features == 'log2':\n",
    "            return int(np.log2(n_features))\n",
    "        elif isinstance(self.max_features, int):\n",
    "            return min(self.max_features, n_features)\n",
    "        elif self.max_features is None:\n",
    "            return n_features\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid max_features: {self.max_features}\")\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train Random Forest\"\"\"\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        if isinstance(y, pd.Series):\n",
    "            y = y.values\n",
    "\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.n_features_ = X.shape[1]\n",
    "\n",
    "        max_features_per_tree = self._get_max_features(self.n_features_)\n",
    "        self.trees = []\n",
    "\n",
    "        print(f\"Training Random Forest with {self.n_estimators} trees...\")\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            X_bootstrap, y_bootstrap = self._bootstrap_sample(X, y)\n",
    "\n",
    "            tree = DecisionTreeClassifier(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_leaf=self.min_samples_leaf,\n",
    "                max_features=max_features_per_tree,\n",
    "                random_state=self.random_state + i if self.random_state is not None else None\n",
    "            )\n",
    "\n",
    "            tree.fit(X_bootstrap, y_bootstrap)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "            if (i + 1) % 10 == 0 or i == 0:\n",
    "                print(f\"  Trained {i + 1}/{self.n_estimators} trees\")\n",
    "\n",
    "        print(\"Random Forest training completed!\")\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict using majority voting\"\"\"\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "        tree_predictions = np.array([tree.predict(X) for tree in self.trees])\n",
    "\n",
    "        predictions = []\n",
    "        for i in range(n_samples):\n",
    "            sample_votes = tree_predictions[:, i]\n",
    "            vote_counts = Counter(sample_votes)\n",
    "            majority_class = vote_counts.most_common(1)[0][0]\n",
    "            predictions.append(majority_class)\n",
    "\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost Implementation From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoostClassifier:\n",
    "    \"\"\"AdaBoost using decision stumps as weak learners\"\"\"\n",
    "\n",
    "    def __init__(self, n_estimators=50, learning_rate=1.0, random_state=42):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.random_state = random_state\n",
    "        self.estimators_ = []\n",
    "        self.estimator_weights_ = []\n",
    "        self.classes_ = None\n",
    "        self.n_features_ = None\n",
    "\n",
    "        if random_state is not None:\n",
    "            np.random.seed(random_state)\n",
    "\n",
    "    def _make_estimator(self):\n",
    "        \"\"\"Create decision stump (depth=1 tree)\"\"\"\n",
    "        return DecisionTreeClassifier(\n",
    "            max_depth=1,\n",
    "            min_samples_leaf=1,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train AdaBoost using SAMME algorithm\"\"\"\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        if isinstance(y, pd.Series):\n",
    "            y = y.values\n",
    "\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.n_classes_ = len(self.classes_)\n",
    "        self.n_features_ = X.shape[1]\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        # Label encoding\n",
    "        self.label_to_int = {label: i for i, label in enumerate(self.classes_)}\n",
    "        self.int_to_label = {i: label for label, i in self.label_to_int.items()}\n",
    "\n",
    "        y_int = np.array([self.label_to_int[label] for label in y])\n",
    "\n",
    "        # Initialize uniform sample weights\n",
    "        sample_weight = np.ones(n_samples) / n_samples\n",
    "\n",
    "        print(f\"Training AdaBoost with {self.n_estimators} weak learners...\")\n",
    "\n",
    "        self.estimators_ = []\n",
    "        self.estimator_weights_ = []\n",
    "\n",
    "        for iboost in range(self.n_estimators):\n",
    "            sample_weight, estimator_weight, estimator_error = self._boost(\n",
    "                X, y_int, sample_weight, iboost\n",
    "            )\n",
    "\n",
    "            if sample_weight is None:\n",
    "                break\n",
    "\n",
    "            self.estimator_weights_.append(estimator_weight)\n",
    "\n",
    "            if estimator_error == 0:\n",
    "                break\n",
    "\n",
    "            if (iboost + 1) % 10 == 0 or iboost == 0:\n",
    "                print(f\"  Trained {iboost + 1}/{self.n_estimators} estimators, \"\n",
    "                      f\"error: {estimator_error:.3f}, weight: {estimator_weight:.3f}\")\n",
    "\n",
    "        print(\"AdaBoost training completed!\")\n",
    "        return self\n",
    "\n",
    "    def _boost(self, X, y, sample_weight, iboost):\n",
    "        \"\"\"Single boosting step\"\"\"\n",
    "        estimator = self._make_estimator()\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        # Weighted bootstrap sample\n",
    "        weighted_indices = np.random.choice(\n",
    "            n_samples,\n",
    "            size=n_samples,\n",
    "            replace=True,\n",
    "            p=sample_weight / np.sum(sample_weight)\n",
    "        )\n",
    "\n",
    "        X_weighted = X[weighted_indices]\n",
    "        y_weighted = y[weighted_indices]\n",
    "\n",
    "        y_weighted_orig = np.array([self.int_to_label[yi] for yi in y_weighted])\n",
    "        estimator.fit(X_weighted, y_weighted_orig)\n",
    "\n",
    "        # Predictions on original training set\n",
    "        y_pred_orig = estimator.predict(X)\n",
    "        y_pred = np.array([self.label_to_int[pred] for pred in y_pred_orig])\n",
    "\n",
    "        # Calculate weighted error\n",
    "        incorrect = y_pred != y\n",
    "        estimator_error = np.average(incorrect, weights=sample_weight)\n",
    "\n",
    "        # Check if error is too high\n",
    "        if estimator_error >= 1.0 - (1.0 / self.n_classes_):\n",
    "            return None, None, None\n",
    "\n",
    "        if estimator_error <= 0:\n",
    "            estimator_error = 1e-10\n",
    "\n",
    "        # Calculate estimator weight (SAMME algorithm)\n",
    "        if self.n_classes_ == 2:\n",
    "            estimator_weight = self.learning_rate * 0.5 * np.log(\n",
    "                (1.0 - estimator_error) / estimator_error\n",
    "            )\n",
    "        else:\n",
    "            estimator_weight = self.learning_rate * np.log(\n",
    "                (1.0 - estimator_error) / estimator_error\n",
    "            ) + np.log(self.n_classes_ - 1.0)\n",
    "\n",
    "        self.estimators_.append(estimator)\n",
    "\n",
    "        # Update sample weights\n",
    "        if iboost < self.n_estimators - 1:\n",
    "            sample_weight *= np.exp(estimator_weight * incorrect)\n",
    "            sample_weight /= np.sum(sample_weight)\n",
    "\n",
    "            if np.sum(sample_weight) == 0:\n",
    "                sample_weight = np.ones(len(sample_weight)) / len(sample_weight)\n",
    "\n",
    "        return sample_weight, estimator_weight, estimator_error\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict using weighted voting\"\"\"\n",
    "        decision = self.decision_function(X)\n",
    "        return self.classes_.take(np.argmax(decision, axis=1))\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"Compute weighted votes for each class\"\"\"\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "        decision = np.zeros((n_samples, self.n_classes_))\n",
    "\n",
    "        for estimator, weight in zip(self.estimators_, self.estimator_weights_):\n",
    "            current_pred = estimator.predict(X)\n",
    "\n",
    "            for i, pred in enumerate(current_pred):\n",
    "                class_idx = self.label_to_int[pred]\n",
    "                decision[i, class_idx] += weight\n",
    "\n",
    "        return decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(y_true, y_pred):\n",
    "    \"\"\"Calculate accuracy score\"\"\"\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "def f1_score(y_true, y_pred, average='macro'):\n",
    "    \"\"\"Calculate F1 score\"\"\"\n",
    "    classes = np.unique(np.concatenate([y_true, y_pred]))\n",
    "\n",
    "    if average == 'macro':\n",
    "        f1_scores = []\n",
    "\n",
    "        for cls in classes:\n",
    "            tp = np.sum((y_true == cls) & (y_pred == cls))\n",
    "            fp = np.sum((y_true != cls) & (y_pred == cls))\n",
    "            fn = np.sum((y_true == cls) & (y_pred != cls))\n",
    "\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            f1_scores.append(f1)\n",
    "\n",
    "        return np.mean(f1_scores)\n",
    "\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "def classification_report(y_true, y_pred):\n",
    "    \"\"\"Generate detailed classification report\"\"\"\n",
    "    classes = np.unique(np.concatenate([y_true, y_pred]))\n",
    "\n",
    "    report = \"              precision    recall  f1-score   support\\n\\n\"\n",
    "\n",
    "    total_support = 0\n",
    "    weighted_precision = 0\n",
    "    weighted_recall = 0\n",
    "    weighted_f1 = 0\n",
    "\n",
    "    for cls in classes:\n",
    "        tp = np.sum((y_true == cls) & (y_pred == cls))\n",
    "        fp = np.sum((y_true != cls) & (y_pred == cls))\n",
    "        fn = np.sum((y_true == cls) & (y_pred != cls))\n",
    "        support = np.sum(y_true == cls)\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        weighted_precision += precision * support\n",
    "        weighted_recall += recall * support\n",
    "        weighted_f1 += f1 * support\n",
    "        total_support += support\n",
    "\n",
    "        report += f\"{str(cls):>12} {precision:>9.2f} {recall:>9.2f} {f1:>9.2f} {support:>9}\\n\"\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    if total_support > 0:\n",
    "        weighted_precision /= total_support\n",
    "        weighted_recall /= total_support\n",
    "        weighted_f1 /= total_support\n",
    "\n",
    "    report += \"\\n\"\n",
    "    report += f\"    accuracy                     {accuracy:>9.2f} {total_support:>9}\\n\"\n",
    "    report += f\"   macro avg {weighted_precision:>9.2f} {weighted_recall:>9.2f} {macro_f1:>9.2f} {total_support:>9}\\n\"\n",
    "    report += f\"weighted avg {weighted_precision:>9.2f} {weighted_recall:>9.2f} {weighted_f1:>9.2f} {total_support:>9}\\n\"\n",
    "\n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. How We Run the Experiment\n",
    "Now, let's talk about how we'll use our data and models. We'll follow these steps:\n",
    "1.  **Load the Data**: We'll start by loading our `Data.csv` file.\n",
    "2.  **Prepare the Data**: We'll split it into training, validation, and test sets, as explained before.\n",
    "3.  **Train Our Models**: We'll teach our Decision Tree, Random Forest, and AdaBoost models using the training data.\n",
    "4.  **Check on Validation Data**: We'll see how well each model performs on the validation data. This helps us choose the best settings for our models.\n",
    "5.  **Final Check on Test Data**: Once we're happy, we'll test our best model on the completely new test data. This is the real measure of its performance.\n",
    "6.  **Report the Results**: We'll show you the scores (Accuracy and F1-score) for each model.\n",
    "7.  **Simple Insights**: We'll share some easy-to-understand thoughts about how each personality prediction method works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where the main part of our experiment will run.\n",
    "# It will load the data, train the models, and show the results.\n",
    "\n",
    "def main():\n",
    "    print(\"Starting Personality Prediction Experiment...\")\n",
    "\n",
    "    # Load the Data.csv file\n",
    "    data = pd.read_csv(\"Data.csv\")\n",
    "\n",
    "    # Prepare features (X) and target (y)\n",
    "    target_column = \"personality_type\"\n",
    "    X = data.drop(columns=[target_column])\n",
    "    y = data[target_column]\n",
    "\n",
    "    print(f\"Target variable: {target_column}\")\n",
    "    print(f\"Personality Types: {np.unique(y)}\")\n",
    "    print(f\"How many of each personality type:\")\n",
    "    for cls in np.unique(y):\n",
    "        count = np.sum(y == cls)\n",
    "        print(f\"  {cls}: {count} ({count/len(y)*100:.1f}%)\")\n",
    "\n",
    "    # Split data: 60% train, 30% validation, 10% test\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=0.4, stratify=y, random_state=42\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.25, stratify=y_temp, random_state=42\n",
    "    )\n",
    "\n",
    "    print(f\"\\nOur data is split like this:\")\n",
    "    print(f\"Training: {len(X_train)} people ({len(X_train)/len(data)*100:.1f}%)\")\n",
    "    print(f\"Validation: {len(X_val)} people ({len(X_val)/len(data)*100:.1f}%)\")\n",
    "    print(f\"Test: {len(X_test)} people ({len(X_test)/len(data)*100:.1f}%)\")\n",
    "\n",
    "    # 1. Train Decision Tree\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 1: Teaching the Decision Tree\")\n",
    "    print(\"=\"*60)\n",
    "    dt = DecisionTreeClassifier(random_state=42)\n",
    "    dt.fit(X_train, y_train)\n",
    "\n",
    "    val_pred_dt = dt.predict(X_val)\n",
    "    dt_val_acc = accuracy_score(y_val, val_pred_dt)\n",
    "    dt_val_f1 = f1_score(y_val, val_pred_dt, average='macro')\n",
    "\n",
    "    print(f\"Decision Tree - How well it did on validation data (Accuracy): {dt_val_acc:.3f}\")\n",
    "    print(f\"Decision Tree - How well it did on validation data (F1-score): {dt_val_f1:.3f}\")\n",
    "\n",
    "    # 2. Train Random Forest\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 2: Teaching the Random Forest\")\n",
    "    print(\"=\"*60)\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=50,\n",
    "        max_depth=12,\n",
    "        max_features='sqrt',\n",
    "        min_samples_leaf=2,\n",
    "        random_state=42\n",
    "    )\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    val_pred_rf = rf.predict(X_val)\n",
    "    rf_val_acc = accuracy_score(y_val, val_pred_rf)\n",
    "    rf_val_f1 = f1_score(y_val, val_pred_rf, average='macro')\n",
    "\n",
    "    print(f\"Random Forest - How well it did on validation data (Accuracy): {rf_val_acc:.3f}\")\n",
    "    print(f\"Random Forest - How well it did on validation data (F1-score): {rf_val_f1:.3f}\")\n",
    "\n",
    "    # 3. Train AdaBoost\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 3: Teaching AdaBoost\")\n",
    "    print(\"=\"*60)\n",
    "    ada = AdaBoostClassifier(\n",
    "        n_estimators=20,\n",
    "        learning_rate=1.0,\n",
    "        random_state=42\n",
    "    )\n",
    "    ada.fit(X_train, y_train)\n",
    "\n",
    "    val_pred_ada = ada.predict(X_val)\n",
    "    ada_val_acc = accuracy_score(y_val, val_pred_ada)\n",
    "    ada_val_f1 = f1_score(y_val, val_pred_ada, average='macro')\n",
    "\n",
    "    print(f\"AdaBoost - How well it did on validation data (Accuracy): {ada_val_acc:.3f}\")\n",
    "    print(f\"AdaBoost - How well it did on validation data (F1-score): {ada_val_f1:.3f}\")\n",
    "\n",
    "    # 4. Compare models on validation set\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 4: Comparing Our Models on Validation Data\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    print(\"VALIDATION RESULTS (Higher is Better):\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Decision Tree - Accuracy: {dt_val_acc:.3f}, F1: {dt_val_f1:.3f}\")\n",
    "    print(f\"Random Forest - Accuracy: {rf_val_acc:.3f}, F1: {rf_val_f1:.3f}\")\n",
    "    print(f\"AdaBoost      - Accuracy: {ada_val_acc:.3f}, F1: {ada_val_f1:.3f}\")\n",
    "\n",
    "    val_scores = {\n",
    "        'Decision Tree': dt_val_acc,\n",
    "        'Random Forest': rf_val_acc,\n",
    "        'AdaBoost': ada_val_acc\n",
    "    }\n",
    "\n",
    "    best_model_name = max(val_scores.keys(), key=lambda k: val_scores[k])\n",
    "    print(f\"\\nBased on validation, the best model is: {best_model_name}\")\n",
    "\n",
    "    # 5. Final evaluation on test set\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 5: Final Check on New Data (Test Set)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Retrain on combined training and validation data\n",
    "    X_trainval = pd.concat([X_train, X_val], ignore_index=True)\n",
    "    y_trainval = pd.concat([y_train, y_val], ignore_index=True)\n",
    "\n",
    "    # Final models\n",
    "    final_dt = DecisionTreeClassifier(random_state=42)\n",
    "    final_dt.fit(X_trainval, y_trainval)\n",
    "\n",
    "    final_rf = RandomForestClassifier(\n",
    "        n_estimators=50, max_depth=12, max_features='sqrt',\n",
    "        min_samples_leaf=2, random_state=42\n",
    "    )\n",
    "    final_rf.fit(X_trainval, y_trainval)\n",
    "\n",
    "    final_ada = AdaBoostClassifier(\n",
    "        n_estimators=50, learning_rate=1.0, random_state=42\n",
    "    )\n",
    "    final_ada.fit(X_trainval, y_trainval)\n",
    "\n",
    "    # Test set predictions\n",
    "    test_pred_dt = final_dt.predict(X_test)\n",
    "    test_pred_rf = final_rf.predict(X_test)\n",
    "    test_pred_ada = final_ada.predict(X_test)\n",
    "\n",
    "    # Test set metrics\n",
    "    dt_test_acc = accuracy_score(y_test, test_pred_dt)\n",
    "    rf_test_acc = accuracy_score(y_test, test_pred_rf)\n",
    "    ada_test_acc = accuracy_score(y_test, test_pred_ada)\n",
    "\n",
    "    dt_test_f1 = f1_score(y_test, test_pred_dt, average='macro')\n",
    "    rf_test_f1 = f1_score(y_test, test_pred_rf, average='macro')\n",
    "    ada_test_f1 = f1_score(y_test, test_pred_ada, average='macro')\n",
    "\n",
    "    print(\"FINAL TEST RESULTS (Higher is Better):\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Decision Tree - Accuracy: {dt_test_acc:.3f}, F1: {dt_test_f1:.3f}\")\n",
    "    print(f\"Random Forest - Accuracy: {rf_test_acc:.3f}, F1: {rf_test_f1:.3f}\")\n",
    "    print(f\"AdaBoost      - Accuracy: {ada_test_acc:.3f}, F1: {ada_test_f1:.3f}\")\n",
    "\n",
    "    test_scores = {\n",
    "        'Decision Tree': dt_test_acc,\n",
    "        'Random Forest': rf_test_acc,\n",
    "        'AdaBoost': ada_test_acc\n",
    "    }\n",
    "\n",
    "    best_test_model = max(test_scores.keys(), key=lambda k: test_scores[k])\n",
    "    best_test_acc = test_scores[best_test_model]\n",
    "\n",
    "    print(f\"\\nOn the final test, the best model is: {best_test_model}\")\n",
    "\n",
    "    if best_test_model != 'Decision Tree':\n",
    "        improvement = ((best_test_acc - dt_test_acc) / dt_test_acc) * 100\n",
    "        print(f\"This model showed an improvement of {improvement:.1f}% compared to the basic Decision Tree.\")\n",
    "\n",
    "    # Detailed classification report for best model\n",
    "    print(f\"\\nDetailed Report for {best_test_model}:\")\n",
    "    print(\"-\" * 60)\n",
    "    if best_test_model == 'Decision Tree':\n",
    "        print(classification_report(y_test, test_pred_dt))\n",
    "    elif best_test_model == 'Random Forest':\n",
    "        print(classification_report(y_test, test_pred_rf))\n",
    "    else:\n",
    "        print(classification_report(y_test, test_pred_ada))\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXPERIMENT FINISHED!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Algorithm insights\n",
    "    print(\"\\nQUICK LOOK AT OUR METHODS:\")\n",
    "    print(\"-\" * 25)\n",
    "    print(\"Decision Tree:\")\n",
    "    print(\"  • It's like a simple 'yes/no' game to guess personality.\")\n",
    "    print(\"  • Easy to understand how it makes decisions.\")\n",
    "    print(\"  • Can sometimes get too focused on the training data (overfitting).\")\n",
    "\n",
    "    print(\"\\nRandom Forest:\")\n",
    "    print(\"  • Uses many Decision Trees and combines their guesses.\")\n",
    "    print(\"  • Much better at avoiding overfitting than a single Decision Tree.\")\n",
    "    print(f\"  • We used {rf.n_estimators} trees for this.\")\n",
    "\n",
    "    print(\"\\nAdaBoost:\")\n",
    "    print(\"  • It's a team of simple learners that learn from each other's mistakes.\")\n",
    "    print(\"  • It focuses more on the hard-to-guess personalities.\")\n",
    "    print(f\"  • We used {len(ada.estimators_)} simple learners for this.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Our Findings and What They Mean\n",
    "After running our experiment, we looked at how well each method predicted personality types. Here's a simple summary:\n",
    "\n",
    "### How They Did on Validation Data\n",
    "The validation data helped us see which model was promising before the final test. Generally, the Random Forest and AdaBoost models performed better than the single Decision Tree, which is expected because they are more advanced methods.\n",
    "\n",
    "### How They Did on New (Test) Data\n",
    "The test data is the most important. It showed us how well our chosen model would work on people it has never seen before. The results here confirm which method is truly the strongest for predicting personality types in our dataset.\n",
    "\n",
    "### Comparing the Methods\n",
    "*   **Decision Tree**: Good for understanding, but sometimes too simple for complex tasks.\n",
    "*   **Random Forest**: Often performs very well because it combines many trees, making it more robust.\n",
    "*   **AdaBoost**: Also very strong, especially good at learning from difficult cases.\n",
    "\n",
    "For our personality data, we found that the more advanced methods (Random Forest and AdaBoost) generally gave us better predictions than a single Decision Tree. This shows that combining simple ideas can lead to powerful results in machine learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
